{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/omghag/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/omghag/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/omghag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/omghag/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from bs4) (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.15.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndownloaded the dataset locally through the above links using terminal wget command    \\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\n",
    "#          https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\n",
    "\n",
    "\"\"\"\n",
    "downloaded the dataset locally through the above links using terminal wget command    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'data/amazon_reviews_us_Office_Products_v1_00.tsv.gz', sep='\\t', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2640254, 15)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          1.0    N                 Y   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
       "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
       "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
       "       'review_headline', 'review_body', 'review_date'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str\n",
      "<StringArray>\n",
      "['5', '1', '4', '2', '3', '2015-06-05', '2015-02-11', nan, '2014-02-14']\n",
      "Length: 9, dtype: str\n"
     ]
    }
   ],
   "source": [
    "print(df['star_rating'].dtype)\n",
    "print(df['star_rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "[ 5.  1.  4.  2.  3. nan]\n"
     ]
    }
   ],
   "source": [
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "print(df['star_rating'].dtype)\n",
    "print(df['star_rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['review_body', 'star_rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2640080, 15)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body  star_rating\n",
      "0                                     Great product.          5.0\n",
      "1  What's to say about this commodity item except...          5.0\n",
      "2    Haven't used yet, but I am sure I will like it.          5.0\n",
      "star_rating\n",
      "1.0     306967\n",
      "2.0     138381\n",
      "3.0     193680\n",
      "4.0     418348\n",
      "5.0    1582704\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df[['review_body', 'star_rating']]  # selecting only relevant columns\n",
    "print(df.head(3))\n",
    "print(df['star_rating'].value_counts().sort_index())  # checking distribution of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Relabeling and Sampling\n",
    " \n",
    "First form three classes and print their statistics. Then randomly select 100,000 reviews from the positive and 100,000 reviews from the negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['star_rating'] != 3]  # removing neutral reviews\n",
    "df['sentiment'] = np.where(df['star_rating'] > 3, 1, 0)  # positive:1, negative:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2446400, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    2001052\n",
       "0     445348\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "positive_df = df[df['sentiment'] == 1].sample(n=100000, random_state=random_seed)\n",
    "negative_df = df[df['sentiment'] == 0].sample(n=100000, random_state=random_seed)\n",
    "df = pd.concat([positive_df, negative_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3)\n",
      "sentiment\n",
      "1    100000\n",
      "0    100000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS_MAP = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"amn't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"daren't\": \"dare not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"everyone's\": \"everyone is\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'll've\": \"I will have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"innit\": \"is it not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"ne'er\": \"never\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"o'er\": \"over\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"outta\": \"out of\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"somebody's\": \"somebody is\",\n",
    "    \"someone's\": \"someone is\",\n",
    "    \"something's\": \"something is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"tis\": \"it is\",\n",
    "    \"twas\": \"it was\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"whatcha\": \"what are you\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  I can't do this. She's going to the market. Y'all've been great!\n",
      "Expanded Text:  I cannot do this. She is going to the market. You all have been great!\n"
     ]
    }
   ],
   "source": [
    "def remove_contractions(text):\n",
    "    # Sort contractions by length (longest first) to handle compound contractions\n",
    "    contractions_sorted = sorted(CONTRACTIONS_MAP.keys(), key=len, reverse=True)\n",
    "    \n",
    "    # Build pattern with word boundaries\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_sorted) + r')\\b', \n",
    "                        flags=re.IGNORECASE)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        match_lower = match.lower()\n",
    "        \n",
    "        if match_lower in CONTRACTIONS_MAP:\n",
    "            expanded = CONTRACTIONS_MAP[match_lower]\n",
    "            \n",
    "            # Preserve original capitalization\n",
    "            if match[0].isupper():\n",
    "                expanded = expanded[0].upper() + expanded[1:]\n",
    "            \n",
    "            return expanded\n",
    "        \n",
    "        return match\n",
    "    \n",
    "    # Keep expanding until no more contractions found\n",
    "    prev_text = \"\"\n",
    "    while prev_text != text:\n",
    "        prev_text = text\n",
    "        text = pattern.sub(expand_match, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "sample_text = \"I can't do this. She's going to the market. Y'all've been great!\"\n",
    "print(\"Original Text: \", sample_text)\n",
    "expanded_text = remove_contractions(sample_text)\n",
    "print(\"Expanded Text: \", expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = remove_contractions(text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(318.00717)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_length_before = df['review_body'].str.len().mean()\n",
    "avg_length_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_body'] = df['review_body'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(302.08331)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_length_after = df['review_body'].str.len().mean()\n",
    "avg_length_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049807    just as advertised and quickly shipped very pl...\n",
      "2439572    this fountain pen has an great feel to it heav...\n",
      "673985     i have order this replacement toner several ti...\n",
      "Name: review_body, dtype: str\n"
     ]
    }
   ],
   "source": [
    "print(df['review_body'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert treebank POS tags to WordNet POS tags\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize_with_pos(text):\n",
    "    \"\"\"\n",
    "    Enhanced lemmatization that tries multiple POS tags for better results\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    \n",
    "    if not words:\n",
    "        return \"\"\n",
    "    \n",
    "    # POS tag the available text\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    lemmatized = []\n",
    "    for word, pos in pos_tags:\n",
    "        # Get primary WordNet POS\n",
    "        primary_pos = get_wordnet_pos(pos)\n",
    "        \n",
    "        # Try lemmatizing with the detected POS\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, primary_pos)\n",
    "        \n",
    "        # If word didn't change and it might be a verb, try verb lemmatization\n",
    "        if lemmatized_word == word and primary_pos != wordnet.VERB:\n",
    "            verb_form = lemmatizer.lemmatize(word, wordnet.VERB)\n",
    "            # Use verb form if it's different (likely was actually a verb)\n",
    "            if verb_form != word:\n",
    "                lemmatized_word = verb_form\n",
    "        \n",
    "        lemmatized.append(lemmatized_word)\n",
    "    \n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: this is not a good product and i do not recommend it\n",
      "After stopword removal: not good product not recommend\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords but keep negation words\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # CRITICAL: Keep negation words for sentiment analysis\n",
    "    negations = {\n",
    "        'no', 'not', 'nor', 'never', 'neither', 'nobody', 'nothing', \n",
    "        'nowhere', 'none', 'hardly', 'scarcely', 'barely'\n",
    "    }\n",
    "    # Remove negation words from stopwords list\n",
    "    stop_words = stop_words - negations\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# Test it on a sample\n",
    "sample_text = \"this is not a good product and i do not recommend it\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"After stopword removal:\", remove_stopwords(sample_text))\n",
    "# Should keep \"not\" in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before preprocessing:\n",
      "1049807    just as advertised and quickly shipped very pl...\n",
      "2439572    this fountain pen has an great feel to it heav...\n",
      "673985     i have order this replacement toner several ti...\n",
      "Name: review_body, dtype: str\n",
      "Average length before preprocessing: 302.0833\n"
     ]
    }
   ],
   "source": [
    "samples_before_preprocessing = df['review_body'].head(3).copy()\n",
    "print(\"Samples before preprocessing:\")\n",
    "print(samples_before_preprocessing)\n",
    "avg_length_before_preprocessing = df['review_body'].str.len().mean()\n",
    "print(f\"Average length before preprocessing:{ avg_length_before_preprocessing: .4f}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before removing stop words:\n",
      "1049807    just as advertised and quickly shipped very pl...\n",
      "2439572    this fountain pen has an great feel to it heav...\n",
      "673985     i have order this replacement toner several ti...\n",
      "Name: review_body, dtype: str\n",
      "Average length before removing stop words: 302.08331\n",
      "Samples after removing stop words:\n",
      "1049807                   advertised quickly shipped pleased\n",
      "2439572    fountain pen great feel heavy not much writing...\n",
      "673985     order replacement toner several times canon sm...\n",
      "Name: review_body, dtype: str\n",
      "Average length after removing stop words: 196.017205\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Save before preprocessing\n",
    "samples_before_stopwords_removal = df['review_body'].head(3).copy()\n",
    "print(\"Samples before removing stop words:\")\n",
    "print(samples_before_stopwords_removal)\n",
    "avg_length_before_stopwords_removal = df['review_body'].str.len().mean()\n",
    "print(\"Average length before removing stop words:\", avg_length_before_stopwords_removal)\n",
    "# Now remove stop words\n",
    "# Apply stopword removal (keeping negations)\n",
    "df['review_body'] = df['review_body'].apply(remove_stopwords)\n",
    "\n",
    "# After all preprocessing\n",
    "samples_after_stopwords_removal = df['review_body'].head(3).copy()\n",
    "print(\"Samples after removing stop words:\")\n",
    "print(samples_after_stopwords_removal)\n",
    "avg_length_after_stopwords_removal = df['review_body'].str.len().mean()\n",
    "print(\"Average length after removing stop words:\", avg_length_after_stopwords_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before lemmatization:\n",
      "1049807                   advertised quickly shipped pleased\n",
      "2439572    fountain pen great feel heavy not much writing...\n",
      "673985     order replacement toner several times canon sm...\n",
      "Name: review_body, dtype: str\n",
      "Average length before lemmatization: 196.017205\n",
      "Samples after lemmatization:\n",
      "1049807                        advertise quickly ship please\n",
      "2439572    fountain pen great feel heavy not much write e...\n",
      "673985     order replacement toner several time canon sma...\n",
      "Name: review_body, dtype: str\n",
      "Average length after lemmatization: 185.24457\n"
     ]
    }
   ],
   "source": [
    "#save before lemmatization\n",
    "samples_before_lemmatization = df['review_body'].head(3).copy()\n",
    "print(\"Samples before lemmatization:\")\n",
    "print(samples_before_lemmatization)\n",
    "avg_length_before_lemmatization = df['review_body'].str.len().mean()\n",
    "print(\"Average length before lemmatization:\", avg_length_before_lemmatization)\n",
    "\n",
    "# Apply lemmatization\n",
    "df['review_body'] = df['review_body'].apply(lemmatize_with_pos)\n",
    "\n",
    "# After lemmatization\n",
    "samples_after_lemmatization = df['review_body'].head(3).copy()\n",
    "print(\"Samples after lemmatization:\")\n",
    "print(samples_after_lemmatization)\n",
    "avg_length_after_lemmatization = df['review_body'].str.len().mean()\n",
    "print(\"Average length after lemmatization:\", avg_length_after_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples after preprocessing:\n",
      "1049807                        advertise quickly ship please\n",
      "2439572    fountain pen great feel heavy not much write e...\n",
      "673985     order replacement toner several time canon sma...\n",
      "Name: review_body, dtype: str\n",
      "Average length after preprocessing:  185.2446\n"
     ]
    }
   ],
   "source": [
    "samples_after_preprocessing = df['review_body'].head(3).copy()\n",
    "print(\"Samples after preprocessing:\")\n",
    "print(samples_after_preprocessing)\n",
    "avg_length_after_preprocessing = df['review_body'].str.len().mean()\n",
    "print(f\"Average length after preprocessing: {avg_length_after_preprocessing: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text: advertise quickly ship please ship\n",
      "Bigram features (tuples): {('advertise', 'quickly'): 1, ('quickly', 'ship'): 1, ('ship', 'please'): 1, ('please', 'ship'): 1}\n",
      "Number of features: 4\n",
      "Type of keys: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk import bigrams\n",
    "\n",
    "def bigram_features_tuple(text):\n",
    "    \"\"\"\n",
    "    Extract bigrams as binary dictionary with TUPLE keys\n",
    "    Exactly like your friend's approach\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 2:\n",
    "        return {}\n",
    "    \n",
    "    # Create bigrams and use tuples as dictionary keys\n",
    "    bigram_list = bigrams(words)\n",
    "    return {bg: 1 for bg in bigram_list}\n",
    "\n",
    "test_text = \"advertise quickly ship please ship\"\n",
    "print(\"Test text:\", test_text)\n",
    "result = bigram_features_tuple(test_text)\n",
    "print(\"Bigram features (tuples):\", result)\n",
    "print(f\"Number of features: {len(result)}\")\n",
    "print(f\"Type of keys: {type(list(result.keys())[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting bigram features with tuple keys...\n",
      "This may take a minute...\n",
      "\n",
      "Done! Checking samples:\n",
      "\n",
      "First review bigrams:\n",
      "{('advertise', 'quickly'): 1, ('quickly', 'ship'): 1, ('ship', 'please'): 1}\n",
      "\n",
      "Number of bigrams in first review: 3\n",
      "\n",
      "Second review bigrams:\n",
      "{('fountain', 'pen'): 1, ('pen', 'great'): 1, ('great', 'feel'): 1, ('feel', 'heavy'): 1, ('heavy', 'not'): 1}\n",
      "Number of bigrams in second review: 44\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['bigrams_tuple'] = df['review_body'].apply(bigram_features_tuple)\n",
    "\n",
    "print(\"\\nDone! Checking samples:\")\n",
    "print(\"\\nFirst review bigrams:\")\n",
    "print(df['bigrams_tuple'].iloc[0])\n",
    "print(f\"\\nNumber of bigrams in first review: {len(df['bigrams_tuple'].iloc[0])}\")\n",
    "\n",
    "print(\"\\nSecond review bigrams:\")\n",
    "print(dict(list(df['bigrams_tuple'].iloc[1].items())[:5]))  # Show first 5\n",
    "print(f\"Number of bigrams in second review: {len(df['bigrams_tuple'].iloc[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting and transforming...\n",
      "\n",
      "Feature matrix shape: (200000, 1680349)\n",
      "Number of samples: 200000\n",
      "Number of unique bigram features: 1680349\n",
      "Matrix type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Sparsity: 99.9983%\n",
      "\n",
      "Sample feature names (first 10):\n",
      "  0: ('I', 'I')\n",
      "  1: ('I', 'abandon')\n",
      "  2: ('I', 'abd')\n",
      "  3: ('I', 'able')\n",
      "  4: ('I', 'absolute')\n",
      "  5: ('I', 'absolutely')\n",
      "  6: ('I', 'absolutley')\n",
      "  7: ('I', 'abuse')\n",
      "  8: ('I', 'academic')\n",
      "  9: ('I', 'accept')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer_tuple = DictVectorizer(sparse=True)\n",
    "\n",
    "# Convert to sparse matrix\n",
    "X_tuple = vectorizer_tuple.fit_transform(df['bigrams_tuple'])\n",
    "y = df['sentiment'].values\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_tuple.shape}\")\n",
    "print(f\"Number of samples: {X_tuple.shape[0]}\")\n",
    "print(f\"Number of unique bigram features: {X_tuple.shape[1]}\")\n",
    "print(f\"Matrix type: {type(X_tuple)}\")\n",
    "print(f\"Sparsity: {(1 - X_tuple.nnz / (X_tuple.shape[0] * X_tuple.shape[1])) * 100:.4f}%\")\n",
    "\n",
    "# Check some feature names\n",
    "print(f\"\\nSample feature names (first 10):\")\n",
    "feature_names = vectorizer_tuple.get_feature_names_out()\n",
    "for i in range(min(10, len(feature_names))):\n",
    "    print(f\"  {i}: {feature_names[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (160000, 1680349)\n",
      "Testing set: (40000, 1680349)\n",
      "Training labels: (160000,)\n",
      "Testing labels: (40000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tuple, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "print(f\"Training labels: {y_train.shape}\")\n",
    "print(f\"Testing labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Training Accuracy: 0.9945\n",
      "Perceptron Training Precision: 0.9910\n",
      "Perceptron Training Recall: 0.9981\n",
      "Perceptron Training F1-score: 0.9946\n",
      "Perceptron Testing Accuracy: 0.8828\n",
      "Perceptron Testing Precision: 0.8781\n",
      "Perceptron Testing Recall: 0.8889\n",
      "Perceptron Testing F1-score: 0.8835\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load and train Perceptron model\n",
    "perceptron = Perceptron(random_state=42, max_iter=10000)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = perceptron.predict(X_train)\n",
    "y_test_pred = perceptron.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Testing metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Perceptron Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Perceptron Training Precision: {train_prec:.4f}\")\n",
    "print(f\"Perceptron Training Recall: {train_rec:.4f}\")\n",
    "print(f\"Perceptron Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"Perceptron Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Perceptron Testing Precision: {test_prec:.4f}\")\n",
    "print(f\"Perceptron Testing Recall: {test_rec:.4f}\")\n",
    "print(f\"Perceptron Testing F1-score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Accuracy: 0.9890\n",
      "SVM Training Precision: 0.9803\n",
      "SVM Training Recall: 0.9981\n",
      "SVM Training F1-score: 0.9891\n",
      "SVM Testing Accuracy: 0.8822\n",
      "SVM Testing Precision: 0.8535\n",
      "SVM Testing Recall: 0.9226\n",
      "SVM Testing F1-score: 0.8867\n"
     ]
    }
   ],
   "source": [
    "#Load and train SVM model\n",
    "svm = LinearSVC(random_state=42, max_iter=10000, C=0.1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = svm.predict(X_train)\n",
    "y_test_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Testing metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"SVM Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"SVM Training Precision: {train_prec:.4f}\")\n",
    "print(f\"SVM Training Recall: {train_rec:.4f}\")\n",
    "print(f\"SVM Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"SVM Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"SVM Testing Precision: {test_prec:.4f}\")\n",
    "print(f\"SVM Testing Recall: {test_rec:.4f}\")\n",
    "print(f\"SVM Testing F1-score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Accuracy: 0.9871\n",
      "Logistic Regression Training Precision: 0.9772\n",
      "Logistic Regression Training Recall: 0.9974\n",
      "Logistic Regression Training F1-score: 0.9872\n",
      "Logistic Regression Testing Accuracy: 0.8870\n",
      "Logistic Regression Testing Precision: 0.8590\n",
      "Logistic Regression Testing Recall: 0.9257\n",
      "Logistic Regression Testing F1-score: 0.8911\n"
     ]
    }
   ],
   "source": [
    "# load and train Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Testing metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Logistic Regression Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Logistic Regression Training Precision: {train_prec:.4f}\")\n",
    "print(f\"Logistic Regression Training Recall: {train_rec:.4f}\")\n",
    "print(f\"Logistic Regression Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"Logistic Regression Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Logistic Regression Testing Precision: {test_prec:.4f}\")\n",
    "print(f\"Logistic Regression Testing Recall: {test_rec:.4f}\")\n",
    "print(f\"Logistic Regression Testing F1-score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy: 0.9693\n",
      "Naive Bayes Training Precision: 0.9747\n",
      "Naive Bayes Training Recall: 0.9636\n",
      "Naive Bayes Training F1-score: 0.9691\n",
      "Naive Bayes Testing Accuracy: 0.8855\n",
      "Naive Bayes Testing Precision: 0.8785\n",
      "Naive Bayes Testing Recall: 0.8947\n",
      "Naive Bayes Testing F1-score: 0.8866\n"
     ]
    }
   ],
   "source": [
    "# load and train Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = nb.predict(X_train)\n",
    "y_test_pred = nb.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Testing metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Naive Bayes Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Naive Bayes Training Precision: {train_prec:.4f}\")\n",
    "print(f\"Naive Bayes Training Recall: {train_rec:.4f}\")\n",
    "print(f\"Naive Bayes Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"Naive Bayes Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Naive Bayes Testing Precision: {test_prec:.4f}\")\n",
    "print(f\"Naive Bayes Testing Recall: {test_rec:.4f}\")\n",
    "print(f\"Naive Bayes Testing F1-score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
