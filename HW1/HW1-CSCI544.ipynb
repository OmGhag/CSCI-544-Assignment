{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/omghag/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/omghag/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/omghag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/omghag/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from bs4) (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.15.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndownloaded the dataset locally through the above links using terminal wget command    \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\n",
    "#          https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\n",
    "\n",
    "\"\"\"\n",
    "downloaded the dataset locally through the above links using terminal wget command    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'data/amazon_reviews_us_Office_Products_v1_00.tsv.gz', sep='\\t', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2640254, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          1.0    N                 Y   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
       "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
       "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
       "       'review_headline', 'review_body', 'review_date'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str\n",
      "<ArrowStringArray>\n",
      "['5', '1', '4', '2', '3', '2015-06-05', '2015-02-11', nan, '2014-02-14']\n",
      "Length: 9, dtype: str\n"
     ]
    }
   ],
   "source": [
    "print(df['star_rating'].dtype)\n",
    "print(df['star_rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "[ 5.  1.  4.  2.  3. nan]\n"
     ]
    }
   ],
   "source": [
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "print(df['star_rating'].dtype)\n",
    "print(df['star_rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['review_body', 'star_rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2640080, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body  star_rating\n",
      "0                                     Great product.          5.0\n",
      "1  What's to say about this commodity item except...          5.0\n",
      "2    Haven't used yet, but I am sure I will like it.          5.0\n",
      "star_rating\n",
      "1.0     306967\n",
      "2.0     138381\n",
      "3.0     193680\n",
      "4.0     418348\n",
      "5.0    1582704\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df[['review_body', 'star_rating']]  # selecting only relevant columns\n",
    "print(df.head(3))\n",
    "print(df['star_rating'].value_counts().sort_index())  # checking distribution of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Relabeling and Sampling\n",
    " \n",
    "First form three classes and print their statistics. Then randomly select 100,000 reviews from the positive and 100,000 reviews from the negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['star_rating'] != 3]  # removing neutral reviews\n",
    "df['sentiment'] = np.where(df['star_rating'] > 3, 1, 0)  # positive:1, negative:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2446400, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    2001052\n",
       "0     445348\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "positive_df = df[df['sentiment'] == 1].sample(n=100000, random_state=random_seed)\n",
    "negative_df = df[df['sentiment'] == 0].sample(n=100000, random_state=random_seed)\n",
    "df = pd.concat([positive_df, negative_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 3)\n",
      "sentiment\n",
      "1    100000\n",
      "0    100000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(318.00717)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_length_before = df['review_body'].str.len().mean()\n",
    "avg_length_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_body'] = df['review_body'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(301.77087)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_length_after = df['review_body'].str.len().mean()\n",
    "avg_length_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049807    just as advertised and quickly shipped very pl...\n",
      "2439572    this fountain pen has an great feel to it heav...\n",
      "673985     i have order this replacement toner several ti...\n",
      "Name: review_body, dtype: str\n"
     ]
    }
   ],
   "source": [
    "print(df['review_body'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert treebank POS tags to WordNet POS tags\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: this is not a good product and i do not recommend it\n",
      "After stopword removal: not good product not recommend\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords but keep negation words\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # CRITICAL: Keep negation words for sentiment analysis\n",
    "    negations = {\n",
    "        'no', 'not', 'nor', 'never', 'neither', 'nobody', 'nothing', \n",
    "        'nowhere', 'none', 'hardly', 'scarcely', 'barely'\n",
    "    }\n",
    "    # Remove negation words from stopwords list\n",
    "    stop_words = stop_words - negations\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# Test it on a sample\n",
    "sample_text = \"this is not a good product and i do not recommend it\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"After stopword removal:\", remove_stopwords(sample_text))\n",
    "# Should keep \"not\" in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before preprocessing:\n",
      "1049807    just as advertised and quickly shipped very pl...\n",
      "2439572    this fountain pen has an great feel to it heav...\n",
      "673985     i have order this replacement toner several ti...\n",
      "Name: review_body, dtype: str\n",
      "Average length before preprocessing: 301.7709\n"
     ]
    }
   ],
   "source": [
    "samples_before_preprocessing = df['review_body'].head(3).copy()\n",
    "print(\"Samples before preprocessing:\")\n",
    "print(samples_before_preprocessing)\n",
    "avg_length_before_preprocessing = df['review_body'].str.len().mean()\n",
    "print(f\"Average length before preprocessing:{ avg_length_before_preprocessing: .4f}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before removing stop words:\n",
      "1049807    just as advertised and quickly shipped very pl...\n",
      "2439572    this fountain pen has an great feel to it heav...\n",
      "673985     i have order this replacement toner several ti...\n",
      "Name: review_body, dtype: str\n",
      "Average length before removing stop words: 301.77087\n",
      "Samples after removing stop words:\n",
      "1049807                   advertised quickly shipped pleased\n",
      "2439572    fountain pen great feel heavy not much writing...\n",
      "673985     order replacement toner several times canon sm...\n",
      "Name: review_body, dtype: str\n",
      "Average length after removing stop words: 195.60442\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Save before preprocessing\n",
    "samples_before_stopwords_removal = df['review_body'].head(3).copy()\n",
    "print(\"Samples before removing stop words:\")\n",
    "print(samples_before_stopwords_removal)\n",
    "avg_length_before_stopwords_removal = df['review_body'].str.len().mean()\n",
    "print(\"Average length before removing stop words:\", avg_length_before_stopwords_removal)\n",
    "# Now remove stop words\n",
    "# Apply stopword removal (keeping negations)\n",
    "df['review_body'] = df['review_body'].apply(remove_stopwords)\n",
    "\n",
    "# After all preprocessing\n",
    "samples_after_stopwords_removal = df['review_body'].head(3).copy()\n",
    "print(\"Samples after removing stop words:\")\n",
    "print(samples_after_stopwords_removal)\n",
    "avg_length_after_stopwords_removal = df['review_body'].str.len().mean()\n",
    "print(\"Average length after removing stop words:\", avg_length_after_stopwords_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_with_pos(text):\n",
    "    \"\"\"Lemmatize text with POS tagging\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    # Get POS tags for all words\n",
    "    pos_tags = pos_tag(words)\n",
    "    # Lemmatize each word with its POS tag\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before lemmatization:\n",
      "1049807                   advertised quickly shipped pleased\n",
      "2439572    fountain pen great feel heavy not much writing...\n",
      "673985     order replacement toner several times canon sm...\n",
      "Name: review_body, dtype: str\n",
      "Average length before lemmatization: 195.60442\n",
      "Samples after lemmatization:\n",
      "1049807                      advertised quickly ship pleased\n",
      "2439572    fountain pen great feel heavy not much write e...\n",
      "673985     order replacement toner several time canon sma...\n",
      "Name: review_body, dtype: str\n",
      "Average length after lemmatization: 185.87826\n"
     ]
    }
   ],
   "source": [
    "#save before lemmatization\n",
    "samples_before_lemmatization = df['review_body'].head(3).copy()\n",
    "print(\"Samples before lemmatization:\")\n",
    "print(samples_before_lemmatization)\n",
    "avg_length_before_lemmatization = df['review_body'].str.len().mean()\n",
    "print(\"Average length before lemmatization:\", avg_length_before_lemmatization)\n",
    "\n",
    "# Apply lemmatization\n",
    "df['review_body'] = df['review_body'].apply(lemmatize_with_pos)\n",
    "\n",
    "# After lemmatization\n",
    "samples_after_lemmatization = df['review_body'].head(3).copy()\n",
    "print(\"Samples after lemmatization:\")\n",
    "print(samples_after_lemmatization)\n",
    "avg_length_after_lemmatization = df['review_body'].str.len().mean()\n",
    "print(\"Average length after lemmatization:\", avg_length_after_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples after preprocessing:\n",
      "1049807                      advertised quickly ship pleased\n",
      "2439572    fountain pen great feel heavy not much write e...\n",
      "673985     order replacement toner several time canon sma...\n",
      "Name: review_body, dtype: str\n",
      "Average length after preprocessing:  185.8783\n"
     ]
    }
   ],
   "source": [
    "samples_after_preprocessing = df['review_body'].head(3).copy()\n",
    "print(\"Samples after preprocessing:\")\n",
    "print(samples_after_preprocessing)\n",
    "avg_length_after_preprocessing = df['review_body'].str.len().mean()\n",
    "print(f\"Average length after preprocessing: {avg_length_after_preprocessing: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any empty bigrams? 6018\n",
      "Sample bigram: advertised_quickly quickly_ship ship_pleased\n",
      "Feature matrix shape: (200000, 1742717)\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_bigrams(text):\n",
    "    words = text.split()\n",
    "    if len(words) < 2:\n",
    "        return \"\"  # Return empty string if can't form bigrams\n",
    "    bigram_list = list(bigrams(words))\n",
    "    # Join each bigram with underscore\n",
    "    bigram_strings = ['_'.join(bigram) for bigram in bigram_list]\n",
    "    return ' '.join(bigram_strings)\n",
    "\n",
    "# Re-apply to all reviews\n",
    "df['bigrams'] = df['review_body'].apply(extract_bigrams)\n",
    "\n",
    "# Check for any issues\n",
    "print(f\"Any empty bigrams? {(df['bigrams'] == '').sum()}\")\n",
    "print(f\"Sample bigram: {df['bigrams'].iloc[0]}\")\n",
    "\n",
    "# Now use CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['bigrams'])\n",
    "y = df['sentiment']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (160000, 1742717)\n",
      "Testing set: (40000, 1742717)\n",
      "Training labels: (160000,)\n",
      "Testing labels: (40000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "print(f\"Training labels: {y_train.shape}\")\n",
    "print(f\"Testing labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Training Accuracy: 0.9949\n",
      "Perceptron Training Precision: 0.9908\n",
      "Perceptron Training Recall: 0.9991\n",
      "Perceptron Training F1-score: 0.9949\n",
      "Perceptron Testing Accuracy: 0.8815\n",
      "Perceptron Testing Precision: 0.8727\n",
      "Perceptron Testing Recall: 0.8932\n",
      "Perceptron Testing F1-score: 0.8829\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load and train Perceptron model\n",
    "perceptron = Perceptron(random_state=42, max_iter=10000)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = perceptron.predict(X_train)\n",
    "y_test_pred = perceptron.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Testing metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Perceptron Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Perceptron Training Precision: {train_prec:.4f}\")\n",
    "print(f\"Perceptron Training Recall: {train_rec:.4f}\")\n",
    "print(f\"Perceptron Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"Perceptron Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Perceptron Testing Precision: {test_prec:.4f}\")\n",
    "print(f\"Perceptron Testing Recall: {test_rec:.4f}\")\n",
    "print(f\"Perceptron Testing F1-score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Accuracy: 0.9952\n",
      "SVM Training Precision: 0.9907\n",
      "SVM Training Recall: 0.9997\n",
      "SVM Training F1-score: 0.9952\n",
      "SVM Testing Accuracy: 0.8702\n",
      "SVM Testing Precision: 0.8460\n",
      "SVM Testing Recall: 0.9050\n",
      "SVM Testing F1-score: 0.8745\n"
     ]
    }
   ],
   "source": [
    "#Load and train SVM model\n",
    "svm = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = svm.predict(X_train)\n",
    "y_test_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Testing metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"SVM Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"SVM Training Precision: {train_prec:.4f}\")\n",
    "print(f\"SVM Training Recall: {train_rec:.4f}\")\n",
    "print(f\"SVM Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"SVM Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"SVM Testing Precision: {test_prec:.4f}\")\n",
    "print(f\"SVM Testing Recall: {test_rec:.4f}\")\n",
    "print(f\"SVM Testing F1-score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Accuracy: 0.9870\n",
      "Logistic Regression Training Precision: 0.9772\n",
      "Logistic Regression Training Recall: 0.9973\n",
      "Logistic Regression Training F1-score: 0.9872\n",
      "Logistic Regression Testing Accuracy: 0.8845\n",
      "Logistic Regression Testing Precision: 0.8561\n",
      "Logistic Regression Testing Recall: 0.9242\n",
      "Logistic Regression Testing F1-score: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# load and train Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Testing metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Logistic Regression Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Logistic Regression Training Precision: {train_prec:.4f}\")\n",
    "print(f\"Logistic Regression Training Recall: {train_rec:.4f}\")\n",
    "print(f\"Logistic Regression Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"Logistic Regression Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Logistic Regression Testing Precision: {test_prec:.4f}\")\n",
    "print(f\"Logistic Regression Testing Recall: {test_rec:.4f}\")\n",
    "print(f\"Logistic Regression Testing F1-score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy: 0.9697\n",
      "Naive Bayes Training Precision: 0.9751\n",
      "Naive Bayes Training Recall: 0.9639\n",
      "Naive Bayes Training F1-score: 0.9695\n",
      "Naive Bayes Testing Accuracy: 0.8833\n",
      "Naive Bayes Testing Precision: 0.8745\n",
      "Naive Bayes Testing Recall: 0.8949\n",
      "Naive Bayes Testing F1-score: 0.8846\n"
     ]
    }
   ],
   "source": [
    "# load and train Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "#Predictions\n",
    "y_train_pred = nb.predict(X_train)\n",
    "y_test_pred = nb.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_prec = precision_score(y_train, y_train_pred)\n",
    "train_rec = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Testing metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_prec = precision_score(y_test, y_test_pred)\n",
    "test_rec = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Naive Bayes Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Naive Bayes Training Precision: {train_prec:.4f}\")\n",
    "print(f\"Naive Bayes Training Recall: {train_rec:.4f}\")\n",
    "print(f\"Naive Bayes Training F1-score: {train_f1:.4f}\")\n",
    "print(f\"Naive Bayes Testing Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Naive Bayes Testing Precision: {test_prec:.4f}\")\n",
    "print(f\"Naive Bayes Testing Recall: {test_rec:.4f}\")\n",
    "print(f\"Naive Bayes Testing F1-score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
