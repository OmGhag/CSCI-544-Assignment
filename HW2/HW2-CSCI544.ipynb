{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed114291",
   "metadata": {},
   "source": [
    "CSCI 544 - Homework 2 <br>\n",
    "Neural Networks for Sentiment Analysis <br>\n",
    "Python Version: 3.12 <br>\n",
    "Library: PyTorch <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf55a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import multiprocessing\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, bigrams\n",
    "\n",
    "# Gensim for Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4777d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from bs4) (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/omghag/CSCI-544-Assignment/.venv/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.15.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndownloaded the dataset locally through the above links using terminal wget command    \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\n",
    "#          https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\n",
    "\n",
    "\"\"\"\n",
    "downloaded the dataset locally through the above links using terminal wget command    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49332d5",
   "metadata": {},
   "source": [
    "# Question 1: Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789ccb3",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697dc82",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a99bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'data/amazon_reviews_us_Office_Products_v1_00.tsv.gz', sep='\\t', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dafe2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2640254, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2313b522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          1.0    N                 Y   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439a171",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e59124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
       "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
       "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
       "       'review_headline', 'review_body', 'review_date'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3a1fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str\n",
      "<StringArray>\n",
      "['5', '1', '4', '2', '3', '2015-06-05', '2015-02-11', nan, '2014-02-14']\n",
      "Length: 9, dtype: str\n"
     ]
    }
   ],
   "source": [
    "print(df['star_rating'].dtype)\n",
    "print(df['star_rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74b6510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "[ 5.  1.  4.  2.  3. nan]\n"
     ]
    }
   ],
   "source": [
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "print(df['star_rating'].dtype)\n",
    "print(df['star_rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d74cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['review_body', 'star_rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c49102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2640080, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e010137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body  star_rating\n",
      "0                                     Great product.          5.0\n",
      "1  What's to say about this commodity item except...          5.0\n",
      "2    Haven't used yet, but I am sure I will like it.          5.0\n",
      "star_rating\n",
      "1.0     306967\n",
      "2.0     138381\n",
      "3.0     193680\n",
      "4.0     418348\n",
      "5.0    1582704\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df[['review_body', 'star_rating']]  # selecting only relevant columns\n",
    "print(df.head(3))\n",
    "print(df['star_rating'].value_counts().sort_index())  # checking distribution of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe991cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Relabeling and Sampling\n",
    " \n",
    "First form three classes and print their statistics. Then randomly select 250,000 reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5972d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating 1: 50000 reviews sampled\n",
      "Rating 2: 50000 reviews sampled\n",
      "Rating 3: 50000 reviews sampled\n",
      "Rating 4: 50000 reviews sampled\n",
      "Rating 5: 50000 reviews sampled\n"
     ]
    }
   ],
   "source": [
    "balanced_dfs = []\n",
    "\n",
    "for rating in [1, 2, 3, 4, 5]:\n",
    "    rating_df = df[df['star_rating'] == rating]\n",
    "    \n",
    "    if len(rating_df) >= 50000:\n",
    "        sampled = rating_df.sample(n=50000, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        print(f\"Warning: Only {len(rating_df)} reviews available for rating {rating}\")\n",
    "        sampled = rating_df\n",
    "    \n",
    "    balanced_dfs.append(sampled)\n",
    "    print(f\"Rating {rating}: {len(sampled)} reviews sampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7a5406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "star_rating\n",
       "1.0    50000\n",
       "2.0    50000\n",
       "3.0    50000\n",
       "4.0    50000\n",
       "5.0    50000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all\n",
    "df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "print(df_balanced.shape)\n",
    "df_balanced['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a278b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    100000\n",
      "2    100000\n",
      "3     50000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_ternary_label(rating):\n",
    "    \"\"\"\n",
    "    rating > 3 → class 1 (Positive)\n",
    "    rating < 3 → class 2 (Negative)\n",
    "    rating = 3 → class 3 (Neutral)\n",
    "    \"\"\"\n",
    "    if rating > 3:\n",
    "        return 1  # Positive\n",
    "    elif rating < 3:\n",
    "        return 2  # Negative\n",
    "    else:\n",
    "        return 3  # Neutral\n",
    "\n",
    "# Fix your labels\n",
    "df_balanced['label'] = df_balanced['star_rating'].apply(create_ternary_label)\n",
    "print(df_balanced['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d277acc",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa2aed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS_MAP = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"amn't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"daren't\": \"dare not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"everyone's\": \"everyone is\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'll've\": \"I will have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"innit\": \"is it not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"ne'er\": \"never\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"o'er\": \"over\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"outta\": \"out of\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"somebody's\": \"somebody is\",\n",
    "    \"someone's\": \"someone is\",\n",
    "    \"something's\": \"something is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"tis\": \"it is\",\n",
    "    \"twas\": \"it was\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"whatcha\": \"what are you\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48d8ba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  I can't do this. She's going to the market. Y'all've been great!\n",
      "Expanded Text:  I cannot do this. She is going to the market. You all have been great!\n"
     ]
    }
   ],
   "source": [
    "def remove_contractions(text):\n",
    "    # Sort contractions by length (longest first) to handle compound contractions\n",
    "    contractions_sorted = sorted(CONTRACTIONS_MAP.keys(), key=len, reverse=True)\n",
    "    \n",
    "    # Build pattern with word boundaries\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_sorted) + r')\\b', \n",
    "                        flags=re.IGNORECASE)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        match_lower = match.lower()\n",
    "        \n",
    "        if match_lower in CONTRACTIONS_MAP:\n",
    "            expanded = CONTRACTIONS_MAP[match_lower]\n",
    "            \n",
    "            # Preserve original capitalization\n",
    "            if match[0].isupper():\n",
    "                expanded = expanded[0].upper() + expanded[1:]\n",
    "            \n",
    "            return expanded\n",
    "        \n",
    "        return match\n",
    "    \n",
    "    # Keep expanding until no more contractions found\n",
    "    prev_text = \"\"\n",
    "    while prev_text != text:\n",
    "        prev_text = text\n",
    "        text = pattern.sub(expand_match, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "sample_text = \"I can't do this. She's going to the market. Y'all've been great!\"\n",
    "print(\"Original Text: \", sample_text)\n",
    "expanded_text = remove_contractions(sample_text)\n",
    "print(\"Expanded Text: \", expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dae0e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = remove_contractions(text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04d8e6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(341.193312)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_length_before = df_balanced['review_body'].str.len().mean()\n",
    "avg_length_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca84cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['review_body'] = df_balanced['review_body'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6178b84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(324.048708)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_length_after = df_balanced['review_body'].str.len().mean()\n",
    "avg_length_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b27bc896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    i purchased these tabs on a whim to put up som...\n",
      "1       returned it too much garbage involved in setup\n",
      "2    my upholstered living room chairs are not part...\n",
      "Name: review_body, dtype: str\n"
     ]
    }
   ],
   "source": [
    "print(df_balanced['review_body'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735d26b",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91f34482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert treebank POS tags to WordNet POS tags\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f33fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize_with_pos(text):\n",
    "    \"\"\"\n",
    "    Enhanced lemmatization that tries multiple POS tags for better results\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    \n",
    "    if not words:\n",
    "        return \"\"\n",
    "    \n",
    "    # POS tag the available text\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    lemmatized = []\n",
    "    for word, pos in pos_tags:\n",
    "        # Get primary WordNet POS\n",
    "        primary_pos = get_wordnet_pos(pos)\n",
    "        \n",
    "        # Try lemmatizing with the detected POS\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, primary_pos)\n",
    "        \n",
    "        # If word didn't change and it might be a verb, try verb lemmatization\n",
    "        if lemmatized_word == word and primary_pos != wordnet.VERB:\n",
    "            verb_form = lemmatizer.lemmatize(word, wordnet.VERB)\n",
    "            # Use verb form if it's different (likely was actually a verb)\n",
    "            if verb_form != word:\n",
    "                lemmatized_word = verb_form\n",
    "        \n",
    "        lemmatized.append(lemmatized_word)\n",
    "    \n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff66cc",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fefa270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: this is not a good product and i do not recommend it\n",
      "After stopword removal: not good product not recommend\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords but keep negation words\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # CRITICAL: Keep negation words for sentiment analysis\n",
    "    negations = {\n",
    "        'no', 'not', 'nor', 'never', 'neither', 'nobody', 'nothing', \n",
    "        'nowhere', 'none', 'hardly', 'scarcely', 'barely'\n",
    "    }\n",
    "    # Remove negation words from stopwords list\n",
    "    stop_words = stop_words - negations\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "# Test it on a sample\n",
    "sample_text = \"this is not a good product and i do not recommend it\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"After stopword removal:\", remove_stopwords(sample_text))\n",
    "# Should keep \"not\" in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c033fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before preprocessing:\n",
      "0    i purchased these tabs on a whim to put up som...\n",
      "1       returned it too much garbage involved in setup\n",
      "2    my upholstered living room chairs are not part...\n",
      "Name: review_body, dtype: str\n",
      "Average length before preprocessing: 324.0487\n"
     ]
    }
   ],
   "source": [
    "samples_before_preprocessing = df_balanced['review_body'].head(3).copy()\n",
    "print(\"Samples before preprocessing:\")\n",
    "print(samples_before_preprocessing)\n",
    "avg_length_before_preprocessing = df_balanced['review_body'].str.len().mean()\n",
    "print(f\"Average length before preprocessing:{ avg_length_before_preprocessing: .4f}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffa0517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before removing stop words:\n",
      "0    i purchased these tabs on a whim to put up som...\n",
      "1       returned it too much garbage involved in setup\n",
      "2    my upholstered living room chairs are not part...\n",
      "Name: review_body, dtype: str\n",
      "Average length before removing stop words: 324.048708\n",
      "Samples after removing stop words:\n",
      "0    purchased tabs whim put movie posters used fou...\n",
      "1                 returned much garbage involved setup\n",
      "2    upholstered living room chairs not particularl...\n",
      "Name: review_body, dtype: str\n",
      "Average length after removing stop words: 209.308632\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Save before preprocessing\n",
    "samples_before_stopwords_removal = df_balanced['review_body'].head(3).copy()\n",
    "print(\"Samples before removing stop words:\")\n",
    "print(samples_before_stopwords_removal)\n",
    "avg_length_before_stopwords_removal = df_balanced['review_body'].str.len().mean()\n",
    "print(\"Average length before removing stop words:\", avg_length_before_stopwords_removal)\n",
    "# Now remove stop words\n",
    "# Apply stopword removal (keeping negations)\n",
    "df_balanced['review_body'] = df_balanced['review_body'].apply(remove_stopwords)\n",
    "\n",
    "# After all preprocessing\n",
    "samples_after_stopwords_removal = df_balanced['review_body'].head(3).copy()\n",
    "print(\"Samples after removing stop words:\")\n",
    "print(samples_after_stopwords_removal)\n",
    "avg_length_after_stopwords_removal = df_balanced['review_body'].str.len().mean()\n",
    "print(\"Average length after removing stop words:\", avg_length_after_stopwords_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0769f",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f22ae11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before lemmatization:\n",
      "0    purchased tabs whim put movie posters used fou...\n",
      "1                 returned much garbage involved setup\n",
      "2    upholstered living room chairs not particularl...\n",
      "Name: review_body, dtype: str\n",
      "Average length before lemmatization: 209.308632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples after lemmatization:\n",
      "0    purchase tab whim put movie poster use four in...\n",
      "1                    return much garbage involve setup\n",
      "2    upholster live room chair not particularly big...\n",
      "Name: review_body, dtype: str\n",
      "Average length after lemmatization: 197.9258\n"
     ]
    }
   ],
   "source": [
    "#save before lemmatization\n",
    "samples_before_lemmatization = df_balanced['review_body'].head(3).copy()\n",
    "print(\"Samples before lemmatization:\")\n",
    "print(samples_before_lemmatization)\n",
    "avg_length_before_lemmatization = df_balanced['review_body'].str.len().mean()\n",
    "print(\"Average length before lemmatization:\", avg_length_before_lemmatization)\n",
    "\n",
    "# Apply lemmatization\n",
    "df_balanced['review_body'] = df_balanced['review_body'].apply(lemmatize_with_pos)\n",
    "\n",
    "# After lemmatization\n",
    "samples_after_lemmatization = df_balanced['review_body'].head(3).copy()\n",
    "print(\"Samples after lemmatization:\")\n",
    "print(samples_after_lemmatization)\n",
    "avg_length_after_lemmatization = df_balanced['review_body'].str.len().mean()\n",
    "print(\"Average length after lemmatization:\", avg_length_after_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4e4bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples after preprocessing:\n",
      "0    purchase tab whim put movie poster use four in...\n",
      "1                    return much garbage involve setup\n",
      "2    upholster live room chair not particularly big...\n",
      "Name: review_body, dtype: str\n",
      "Average length after preprocessing:  197.9258\n"
     ]
    }
   ],
   "source": [
    "samples_after_preprocessing = df_balanced['review_body'].head(3).copy()\n",
    "print(\"Samples after preprocessing:\")\n",
    "print(samples_after_preprocessing)\n",
    "avg_length_after_preprocessing = df_balanced['review_body'].str.len().mean()\n",
    "print(f\"Average length after preprocessing: {avg_length_after_preprocessing: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac55a9f",
   "metadata": {},
   "source": [
    "# Question 2: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b73da",
   "metadata": {},
   "source": [
    "#### (a) loading pretrained \"word2vec-google-news-300” Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90db0f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "pretrained_w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ab33b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3,000,000\n",
      "Vector dimensionality: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(pretrained_w2v.key_to_index):,}\")\n",
    "print(f\"Vector dimensionality: {pretrained_w2v.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56e1b7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 results:\n",
      "  queen           similarity: 0.7118\n",
      "  monarch         similarity: 0.6190\n",
      "  princess        similarity: 0.5902\n",
      "  crown_prince    similarity: 0.5499\n",
      "  prince          similarity: 0.5377\n"
     ]
    }
   ],
   "source": [
    "# SEMANTIC SIMILARITY TEST 1: King - Man + Woman\n",
    "try:\n",
    "    result = pretrained_w2v.most_similar(\n",
    "        positive=['king', 'woman'], \n",
    "        negative=['man'], \n",
    "        topn=5\n",
    "    )\n",
    "    print(\"\\nTop 5 results:\")\n",
    "    for word, score in result:\n",
    "        print(f\"  {word:15} similarity: {score:.4f}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb72f3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity(excellent, outstanding) = 0.5567\n",
      "\n",
      "Words most similar to 'excellent':\n",
      "  terrific        similarity: 0.7410\n",
      "  superb          similarity: 0.7063\n",
      "  exceptional     similarity: 0.6815\n",
      "  fantastic       similarity: 0.6803\n",
      "  good            similarity: 0.6443\n"
     ]
    }
   ],
   "source": [
    "# SEMANTIC SIMILARITY TEST 2: excellent ~ outstanding\n",
    "try:\n",
    "    similarity = pretrained_w2v.similarity('excellent', 'outstanding')\n",
    "    print(f\"\\nSimilarity(excellent, outstanding) = {similarity:.4f}\")\n",
    "    \n",
    "    # Show most similar words to 'excellent'\n",
    "    print(\"\\nWords most similar to 'excellent':\")\n",
    "    similar_words = pretrained_w2v.most_similar('excellent', topn=5)\n",
    "    for word, score in similar_words:\n",
    "        print(f\"  {word:15} similarity: {score:.4f}\")\n",
    "        \n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f1fb1a",
   "metadata": {},
   "source": [
    "#### (b) Training custom Word2Vec on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94c93b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing tokenized reviews...\n",
      "Number of reviews: 250,000\n",
      "Sample tokenized review:\n",
      "  ['purchase', 'tab', 'whim', 'put', 'movie', 'poster', 'use', 'four', 'inch', 'tab', 'poster', 'not', 'job', 'poster', 'stay', 'day', 'one', 'right', 'start', 'fall']...\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenized reviews for Word2Vec training\n",
    "print(\"\\nPreparing tokenized reviews...\")\n",
    "# Use your preprocessed reviews (already cleaned and lemmatized)\n",
    "tokenized_reviews = [review.split() for review in df_balanced['review_body']]\n",
    "\n",
    "print(f\"Number of reviews: {len(tokenized_reviews):,}\")\n",
    "print(f\"Sample tokenized review:\")\n",
    "print(f\"  {tokenized_reviews[0][:20]}...\")  # Show first 20 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1104d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Vocabulary size: 13,116\n",
      "Vector dimensionality: 300\n",
      "\n",
      "Model saved to 'custom_word2vec.model'\n"
     ]
    }
   ],
   "source": [
    "custom_w2v = Word2Vec(\n",
    "    sentences=tokenized_reviews,\n",
    "    vector_size=300,      # embedding size = 300\n",
    "    window=11,            # window size = 11\n",
    "    min_count=10,         # minimum word count = 10\n",
    "    workers= multiprocessing.cpu_count(),            # use all available CPU cores\n",
    "    seed=RANDOM_STATE,    # for reproducibility\n",
    "    epochs=10,            # training epochs\n",
    "    sg=0,                 # CBOW (0) or Skip-gram (1)\n",
    "    negative=5            # negative sampling\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Vocabulary size: {len(custom_w2v.wv.key_to_index):,}\")\n",
    "print(f\"Vector dimensionality: {custom_w2v.wv.vector_size}\")\n",
    "\n",
    "# Save the model\n",
    "custom_w2v.save('custom_word2vec.model')\n",
    "print(\"\\nModel saved to 'custom_word2vec.model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "864c169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. VOCABULARY SIZE:\n",
      "   Pretrained (Google News): 3,000,000 words\n",
      "   Custom (Office Reviews):  13,116 words\n",
      "   Ratio: 228.7x larger\n"
     ]
    }
   ],
   "source": [
    "# Compare vocabulary sizes\n",
    "print(\"\\n1. VOCABULARY SIZE:\")\n",
    "print(f\"   Pretrained (Google News): {len(pretrained_w2v.key_to_index):,} words\")\n",
    "print(f\"   Custom (Office Reviews):  {len(custom_w2v.wv.key_to_index):,} words\")\n",
    "print(f\"   Ratio: {len(pretrained_w2v.key_to_index) / len(custom_w2v.wv.key_to_index):.1f}x larger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8566490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. TRAINING DATA:\n",
      "   Pretrained: ~100 billion words from Google News\n",
      "   Custom: 250,000 Amazon office product reviews\n"
     ]
    }
   ],
   "source": [
    "# Compare training corpus\n",
    "print(\"\\n2. TRAINING DATA:\")\n",
    "print(\"   Pretrained: ~100 billion words from Google News\")\n",
    "print(\"   Custom: 250,000 Amazon office product reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "656813c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. DOMAIN-SPECIFIC VOCABULARY:\n",
      "\n",
      "   Word            Pretrained      Custom         \n",
      "   ---------------------------------------------\n",
      "   product         ✓               ✓              \n",
      "   quality         ✓               ✓              \n",
      "   price           ✓               ✓              \n",
      "   shipping        ✓               ✗              \n",
      "   recommend       ✓               ✓              \n",
      "   excellent       ✓               ✓              \n",
      "   terrible        ✓               ✓              \n",
      "   refund          ✓               ✓              \n",
      "   packaging       ✓               ✗              \n",
      "   defective       ✓               ✓              \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test domain-specific words\n",
    "print(\"\\n3. DOMAIN-SPECIFIC VOCABULARY:\")\n",
    "test_words = ['product', 'quality', 'price', 'shipping', 'recommend', \n",
    "              'excellent', 'terrible', 'refund', 'packaging', 'defective']\n",
    "\n",
    "print(f\"\\n   {'Word':<15} {'Pretrained':<15} {'Custom':<15}\")\n",
    "print(\"   \" + \"-\" * 45)\n",
    "\n",
    "for word in test_words:\n",
    "    pretrained_exists = word in pretrained_w2v\n",
    "    custom_exists = word in custom_w2v.wv\n",
    "    print(f\"   {word:<15} {'✓' if pretrained_exists else '✗':<15} {'✓' if custom_exists else '✗':<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "789d6294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. SEMANTIC SIMILARITY COMPARISON:\n",
      "   Testing word pairs from our domain:\n",
      "\n",
      "   Word Pair                 Pretrained      Custom         \n",
      "   -------------------------------------------------------\n",
      "   good - excellent            0.6443          0.5739         \n",
      "   bad - terrible             0.6829          0.4758         \n",
      "   buy - purchase             0.7640          0.7492         \n",
      "   product - item                 0.2570          0.4883         \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare semantic similarities\n",
    "print(\"\\n4. SEMANTIC SIMILARITY COMPARISON:\")\n",
    "print(\"   Testing word pairs from our domain:\")\n",
    "\n",
    "word_pairs = [\n",
    "    ('good', 'excellent'),\n",
    "    ('bad', 'terrible'),\n",
    "    ('buy', 'purchase'),\n",
    "    ('product', 'item'),\n",
    "]\n",
    "\n",
    "print(f\"\\n   {'Word Pair':<25} {'Pretrained':<15} {'Custom':<15}\")\n",
    "print(\"   \" + \"-\" * 55)\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    try:\n",
    "        sim_pre = pretrained_w2v.similarity(word1, word2)\n",
    "    except KeyError:\n",
    "        sim_pre = None\n",
    "    \n",
    "    try:\n",
    "        sim_cust = custom_w2v.wv.similarity(word1, word2)\n",
    "    except KeyError:\n",
    "        sim_cust = None\n",
    "    \n",
    "    pre_str = f\"{sim_pre:.4f}\" if sim_pre else \"N/A\"\n",
    "    cust_str = f\"{sim_cust:.4f}\" if sim_cust else \"N/A\"\n",
    "    \n",
    "    print(f\"   {word1} - {word2:<20} {pre_str:<15} {cust_str:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef20c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. OUT-OF-VOCABULARY (OOV) ANALYSIS:\n",
      "   Analyzing how many words from our reviews are missing from each model...\n",
      "\n",
      "   Total words analyzed: 34,605\n",
      "   Pretrained OOV rate: 3.13%\n",
      "   Custom OOV rate: 2.95%\n"
     ]
    }
   ],
   "source": [
    "# Test Out-of-Vocabulary (OOV) rate\n",
    "print(\"\\n5. OUT-OF-VOCABULARY (OOV) ANALYSIS:\")\n",
    "print(\"   Analyzing how many words from our reviews are missing from each model...\")\n",
    "\n",
    "# Sample 1000 reviews\n",
    "sample_reviews = df_balanced['review_body'].sample(1000, random_state=RANDOM_STATE)\n",
    "\n",
    "oov_pretrained = 0\n",
    "oov_custom = 0\n",
    "total_words = 0\n",
    "\n",
    "for review in sample_reviews:\n",
    "    words = review.split()\n",
    "    for word in words:\n",
    "        total_words += 1\n",
    "        if word not in pretrained_w2v:\n",
    "            oov_pretrained += 1\n",
    "        if word not in custom_w2v.wv:\n",
    "            oov_custom += 1\n",
    "\n",
    "print(f\"\\n   Total words analyzed: {total_words:,}\")\n",
    "print(f\"   Pretrained OOV rate: {oov_pretrained/total_words*100:.2f}%\")\n",
    "print(f\"   Custom OOV rate: {oov_custom/total_words*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c8b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
